{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_handling.api import direct_download\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pystan\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (16, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['IVV', 'EFA', 'EEM', 'IEF', 'USIG', 'EMB']\n",
    "data = await direct_download(tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = np.log(data['adj_close'].unstack()).diff().dropna()\n",
    "# Stochastic volatility model assumes zero mean returns process\n",
    "returns -= returns.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EEM', 'EFA', 'EMB', 'IEF', 'IVV', 'USIG'], dtype='object', name='ticker')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returns.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CAPM Equilibrium Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>EEM</th>\n",
       "      <td>0.000085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EFA</th>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EMB</th>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IEF</th>\n",
       "      <td>0.000080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>IVV</th>\n",
       "      <td>0.000084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>USIG</th>\n",
       "      <td>0.000081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0\n",
       "ticker          \n",
       "EEM     0.000085\n",
       "EFA     0.000084\n",
       "EMB     0.000081\n",
       "IEF     0.000080\n",
       "IVV     0.000084\n",
       "USIG    0.000081"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Must transform log returns back to simple returns for calculation. Transforms equilbirum returns to log returns\n",
    "# 1-month T-Bill constant maturity rate\n",
    "# https://fred.stlouisfed.org/series/DGS1MO\n",
    "rf = .0205\n",
    "rf = (1 + rf) ** (1/252) - 1\n",
    "# CAPM equilibrium returns.\n",
    "# Must be in same order as tickers index\n",
    "market_cap = [26.31, 57.94, 15.37, 17.24, 179.43, 3.48]\n",
    "market_cap_weights = market_cap / np.sum(market_cap)\n",
    "portfolio_returns = (np.exp(returns) - 1) @ market_cap_weights\n",
    "# Fabozzi 'Robust Portfolio Optimization and Management' pg 234\n",
    "risk_premium = (np.mean(portfolio_returns) - rf) / np.var(portfolio_returns)\n",
    "equil_return = ((risk_premium * np.cov((np.exp(returns) - 1).dropna().T)) @ market_cap_weights) + rf\n",
    "# Transform into log return\n",
    "equil_return = np.log(equil_return + 1)\n",
    "pd.DataFrame(equil_return, index=returns.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6 Tickers, 4 chains, 1000 iterations, 250 warmup on 4790k: 1.8 hours\n",
    "# 6 Tickers, 4 chains, 2750, 250 warmup, adapt_delt: 0.90 on 4790k: 3.91 hours\n",
    "# 6 Tickers, 4 chains, 1500 iterations, 250 warmup, adapt_delta: 0.90 on 4790k: 2.08 hours\n",
    "# adapt_delta: 0.90 results in significantly better effective samples\n",
    "# 0.2475 R-squared calculated from squared returns as volatility\n",
    "# 'MULTIVARIATE STOCHASTIC VOLATILITY MODELS:BAYESIAN ESTIMATION AND MODEL COMPARISON' Jun Yu, Renate Meyer 2006\n",
    "# Constant Correlation Model\n",
    "model_spec = '''\n",
    "data {\n",
    "    int len;                  // Length of time series\n",
    "    int num;                  // Number of tickers\n",
    "    vector[num] returns[len]; // Matrix of size (len, num)\n",
    "    corr_matrix[num] corr;    // Correlation matrix\n",
    "}\n",
    "transformed data {\n",
    "    cholesky_factor_corr[num] L;\n",
    "    L = cholesky_decompose(corr);\n",
    "}\n",
    "parameters {\n",
    "    vector[num] mu;\n",
    "    vector<lower=-1, upper=1>[num] phi;\n",
    "    vector<lower=0>[num] sigma;\n",
    "    vector[num] h_std[len];\n",
    "}\n",
    "transformed parameters {\n",
    "    vector[num] h[len];        // Distributed normal(mu + phi * (h[t-1] - mu), sigma)\n",
    "    for (t in 1:len) {\n",
    "        if (t == 1) {\n",
    "            h[t] = h_std[t] .* sigma;\n",
    "            h[t] += mu;\n",
    "        }\n",
    "        else {\n",
    "            h[t] = h_std[t] .* sigma;\n",
    "            h[t] += mu + phi .* (h[t-1] - mu);\n",
    "        }\n",
    "    }\n",
    "}\n",
    "model {\n",
    "    mu ~ normal(0, 25);\n",
    "    phi ~ uniform(-1, 1);\n",
    "    sigma ~ inv_gamma(2.5, .1);\n",
    "    \n",
    "    for (t in 1:len) {\n",
    "        h_std[t] ~ std_normal();\n",
    "        // Cholesky of covariance matrix == diag matrix of standard deviations * Cholesky of correlation matrix\n",
    "        returns[t] ~ multi_normal_cholesky(rep_vector(0, num), diag_pre_multiply(exp(h[t] / 2), L));\n",
    "    }\n",
    "}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pystan:COMPILING THE C++ CODE FOR MODEL anon_model_aeb0d0c059ad7094d680b2ca5566f00a NOW.\n"
     ]
    }
   ],
   "source": [
    "model = pystan.StanModel(model_code=model_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_func():\n",
    "    length = len(returns.columns)\n",
    "    init_dict = {'mu': np.full(length, -10), 'phi': np.full(length, 0.90), 'sigma': np.full(length, 0.10)}\n",
    "    return init_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pystan:Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.\n",
      "To run all diagnostics call pystan.check_hmc_diagnostics(fit)\n",
      "WARNING:pystan:19 of 5000 iterations ended with a divergence (0.38 %).\n",
      "WARNING:pystan:Try running with adapt_delta larger than 0.9 to remove the divergences.\n"
     ]
    }
   ],
   "source": [
    "# Multivariate\n",
    "params = {'len': len(returns), 'num': len(returns.columns), 'returns': returns, 'corr': np.corrcoef(returns.T)}\n",
    "\n",
    "control = {'adapt_delta': 0.90}\n",
    "mcmc_sample = model.sampling(data=params, chains=4, warmup=250, iter=1500, control=control, init=init_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = mcmc_sample.extract(pars=['mu', 'phi', 'sigma', 'h'], permuted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves MCMC samples\n",
    "import pickle\n",
    "with open('sample_data', 'wb') as file:\n",
    "    pickle.dump(sample_data, file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggreate data across each chain into single dimension\n",
    "mu = sample_data['mu'].reshape(-1, len(returns.columns))\n",
    "phi = sample_data['phi'].reshape(-1, len(returns.columns))\n",
    "sigma = sample_data['sigma'].reshape(-1, len(returns.columns))\n",
    "# Volatility has additional time dimension\n",
    "h = sample_data['h'].reshape(-1, len(returns), len(returns.columns))\n",
    "\n",
    "# Generate 100,000 random indexes\n",
    "idx = np.random.randint(0, 5000, size=100000)\n",
    "\n",
    "# Get random samples for each parameter\n",
    "mu = mu[idx]\n",
    "phi = phi[idx]\n",
    "sigma = sigma[idx]\n",
    "# Select only last volatility entry, removing time dimension\n",
    "h = h[idx, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward simulation\n",
    "h_sim = np.zeros((21, 100000, 6))\n",
    "returns_sim = np.zeros((21, 100000, 6))\n",
    "cholesky_corr = np.linalg.cholesky(np.corrcoef(returns.T))\n",
    "for t in range(21):\n",
    "    # First iteration use last volatility from MCMC samples\n",
    "    if t == 0:  \n",
    "        h_sim[t] = mu + phi * (h - mu) + np.random.normal(loc=0, scale=sigma, size=(100000, len(returns.columns)))\n",
    "        # First Creates array of size (100000, len, len): sequence of diagonal matrices created from volatility\n",
    "        # diagonal matrix of standard deviations * cholesky of correlation matrix = cholesky of covariance matrix\n",
    "        cholesky_mats = (np.eye(len(returns.columns)) * np.exp(h_sim[t] / 2)[:, np.newaxis, :]) @ cholesky_corr\n",
    "        # Multiples each of those matrices by a vector of standard normal samples to create correlated samples\n",
    "        # Cholesky of covariance * vector of standard normals = correlated multivariate normal\n",
    "        # I really don't understand how einsum works, but this DOES WORK PROPERLY\n",
    "        returns_sim[t] = np.einsum('ijk,ik->ij', cholesky_mats, np.random.normal(loc=0, scale=1, size=(100000, len(returns.columns))))\n",
    "    # Rest of iterations use last simulated volatility sample\n",
    "    else:\n",
    "        h_sim[t] = mu + phi * (h_sim[t - 1] - mu) + np.random.normal(loc=0, scale=sigma, size=(100000, len(returns.columns)))\n",
    "        cholesky_mats = (np.eye(len(returns.columns)) * np.exp(h_sim[t] / 2)[:, np.newaxis, :]) @ cholesky_corr\n",
    "        returns_sim[t] = np.einsum('ijk,ik->ij', cholesky_mats, np.random.normal(loc=0, scale=1, size=(100000, len(returns.columns))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample for optimization.\n",
    "# Add in expected daily return\n",
    "sample = returns_sim + equil_return\n",
    "# Simulated returns are logarithmic so sum accross time for total return over period\n",
    "sample = np.sum(sample, axis=0)\n",
    "# Convert to simple returns\n",
    "sample = np.exp(sample) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cvar(weights, var_level, sample):\n",
    "    \"\"\"\n",
    "    Returns negative CVaR for minimization\n",
    "    weights: vector indicating portfolio weights\n",
    "    var_level: (0, 100) number indicating VaR level to use\n",
    "    sample: (n, m) matrix containing asset returns sample where m is number of assets and n is number of samples\n",
    "    \"\"\"\n",
    "    port_returns = sample @ weights\n",
    "    var = np.percentile(port_returns, (100 - var_level))\n",
    "    return -np.mean(port_returns[port_returns < var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opt_fun(weights, sample):\n",
    "    \"\"\"\n",
    "    Returns negative portfolio return for minimization.\n",
    "    Uses L2 regularization to stabalize portfolio weights across differing samples.\n",
    "    Without regularization weights display high variance from small variances in sampling.\n",
    "    weights: m length vector of portfolio weights where m is number of assets\n",
    "    sample: (n, m) matrix containing asset return samples where m is number of assets and n is number of samples\n",
    "    \"\"\"\n",
    "    port_returns = sample @ weights\n",
    "    # This regularization coefficient makes weights vary within about +- 1% in my testing\n",
    "    return -np.mean(port_returns) + .001 * np.sqrt(np.sum(np.square(weights)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing parameters with partial functions\n",
    "cvar_partial = partial(cvar, var_level=99, sample=sample)\n",
    "opt_fun_partial = partial(opt_fun, sample=sample)\n",
    "# Maximize return subject to CVaR constraint\n",
    "constraints = [{'type': 'eq', 'fun': lambda x: np.sum(x) - 1}, # Sum of weights should be 1\n",
    "               {'type': 'ineq', 'fun': lambda x: .05 - cvar_partial(x)}] # CVaR should be <= to some value\n",
    "bounds = [(0, 1) for x in returns.columns] # Long only\n",
    "guess = np.ones(len(returns.columns)) / len(returns.columns) # Intial guess equal weight\n",
    "# Increased tolerance is needed to handle small daily return values\n",
    "result = minimize(opt_fun_partial, guess, constraints=constraints, bounds=bounds, method='SLSQP', tol=1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-08252bfc1623>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'99% VaR: {round(np.percentile(sample @ result.x, 1) * 100, 2)}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'CVaR: {round(-cvar_partial(result.x) * 100, 2)}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Montly Return: {round(np.mean(sample @ result.x) * 100, 2)}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Annualized Return: {round(((1 + np.mean(sample @ result.x))**12 - 1) * 100, 2)}%'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "print(f'99% VaR: {round(np.percentile(sample @ result.x, 1) * 100, 2)}%')\n",
    "print(f'CVaR: {round(-cvar_partial(result.x) * 100, 2)}%')\n",
    "print(f'Montly Return: {round(np.mean(sample @ result.x) * 100, 2)}%')\n",
    "print(f'Annualized Return: {round(((1 + np.mean(sample @ result.x))**12 - 1) * 100, 2)}%')\n",
    "pd.DataFrame(result.x, index=returns.columns).round(3) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shares Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_value = 5386.91"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ticker\n",
       "EEM     29.0\n",
       "EFA      6.0\n",
       "EMB      8.0\n",
       "IEF     13.0\n",
       "IVV      1.0\n",
       "USIG    14.0\n",
       "Name: 2019-08-07 00:00:00, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_prices = data['adj_close'].unstack().iloc[-1]\n",
    "# Number of shares needed to meet optimized allocation, always rounded down to nearest integer\n",
    "np.floor(result.x * port_value / last_prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
